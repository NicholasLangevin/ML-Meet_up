mod.pca <- glm(as.formula(mod.str), data = dat.pca, family = binomial(link = 'logit'))
summary(mod.pca)
plot(pca.import$importance[3,])
sample(dat, size = 1:nrow(dat), replace = F)
sample(dat.pca, size = 1:nrow(dat), replace = F)
k <- sample(1:nrow(dat.pca),0.7 * nrow(dat.pca), replace = F)
head(k)
train <- dat.pca[k,]
valid <- dat.pca[-k,]
train.mod <- glm(as.formula(mod.str), data = train, family = binomial(link = 'logit'))
valid$Pred <- predict(train.mod, newdata = valid, type = 'response')
valid$Pred
mean(valid$Pred)
max(valid$Pred)
summary(valid$Pred)
library(stats)
install.packages('caret')
library(caret)
install.packages("heuristica")
library(heuristica)
treshold <- 0.3
valid$validbin <- ifelse(valid$Pred >= treshold, 1, 0)
head(valid)
confusionMatrixFor_Neg1_0_1(valid$Ind, valid$validbin)
confusionMatrixFor_Neg1_0_1(valid$Ind, valid$validbin)[-1,-1]
mat <- confusionMatrixFor_Neg1_0_1(valid$Ind, valid$validbin)[-1,-1]
heuristica::statsFromConfusionMatrix(mat)
mat <- confusionMatrixFor_Neg1_0_1(valid$Ind, valid$validbin)[-1,-1]
mat <- confusionMatrixFor_Neg1_0_1(valid$Ind, valid$validbin)
heuristica::statsFromConfusionMatrix(mat)
heuristica::statsFromConfusionMatrix(mat[-1,-1])
mat <- confusionMatrixFor_Neg1_0_1(valid$Ind, valid$validbin)[-1,-1]
(mat <- confusionMatrixFor_Neg1_0_1(valid$Ind, valid$validbin)[-1,-1])
Precision <- mat[2,2] / sum(mat[,2])
(Precision <- mat[2,2] / sum(mat[,2]))
recall <- mat[2,2] / sum(mat[2,])
precision <- mat[2,2] / sum(mat[,2])
recall <- mat[2,2] / sum(mat[2,])
f1_score <- 2 * (precision * recall) / (precision + recall)
f1_score
treshold <- 0.35
valid$Pred <- predict(train.mod, newdata = valid, type = 'response')
valid$validbin <- ifelse(valid$Pred >= treshold, 1, 0)
summary(valid$Pred)
(mat <- confusionMatrixFor_Neg1_0_1(valid$Ind, valid$validbin)[-1,-1])
precision <- mat[2,2] / sum(mat[,2])
recall <- mat[2,2] / sum(mat[2,])
f1_score <- 2 * (precision * recall) / (precision + recall)
data.frame(f1_score, precision, recall)
summary(valid$Pred)
treshold <- 0.22
valid$Pred <- predict(train.mod, newdata = valid, type = 'response')
valid$validbin <- ifelse(valid$Pred >= treshold, 1, 0)
summary(valid$Pred)
(mat <- confusionMatrixFor_Neg1_0_1(valid$Ind, valid$validbin)[-1,-1])
precision <- mat[2,2] / sum(mat[,2])
recall <- mat[2,2] / sum(mat[2,])
f1_score <- 2 * (precision * recall) / (precision + recall)
data.frame(f1_score, precision, recall)
mod.final <- glm(as.formula(mod.str), data = dat.pca, family = binomial(link = 'logit'))
seuil <- seq(0.1, 0.9, 0.1)
(seuil <- seq(0.1, 0.9, 0.1))
seuil <- c(seq(0.1, 0.9, 0.1), seq(0.91, 0.99, 0.01))
seuil
summary(model.pca)
treshold <- seq(0.15, 0.4, 0.01)
f1_score <- matrix(NA, nrow = length(seuil), ncol = length(treshold))
## Séparation des données d'entrainement
k <- sample(1:nrow(dat.pca),0.7 * nrow(dat.pca), replace = F)
train <- dat.pca[k,]
valid <- dat.pca[-k,]
f1_score <- matrix(NA, nrow = length(seuil), ncol = length(treshold))
## Séparation des données d'entrainement
k <- sample(1:nrow(dat.pca),0.7 * nrow(dat.pca), replace = F)
train <- dat.pca[k,]
valid <- dat.pca[-k,]
## Définition des seuils
seuil <- c(seq(0.1, 0.9, 0.1), seq(0.91, 0.99, 0.01))
treshold <- seq(0.15, 0.4, 0.01)
## Séparation des données d'entrainement
k <- sample(1:nrow(dat.pca),0.7 * nrow(dat.pca), replace = F)
train <- dat.pca[k,]
valid <- dat.pca[-k,]
f1_score <- matrix(NA, nrow = length(seuil), ncol = length(treshold))
for (i in length(seuil))
{
for (j in length(treshold))
{
dat.pca <- data.frame(Ind = toitvert$green_roof_ind,
pca.x[, which(pca.summary$importance[3,] <= seuil[i])])
model.str <- 'Ind~.'
model.pca <- glm(as.formula(mod.str), data = dat.pca, family = binomial(link = 'logit'))
summary(model.pca) ; anova(model.pca)
train.mod <- glm(as.formula(mod.str), data = train, family = binomial(link = 'logit'))
## Calcul des prédictions et de la conversion
valid$Pred <- predict(train.mod, newdata = valid, type = 'response')
valid$validbin <- ifelse(valid$Pred >= treshold[j], 1, 0)
## Générer la matrice des confusions et calculer les statistiques pertinentes
confusion_mat <- confusionMatrixFor_Neg1_0_1(valid$Ind, valid$validbin)[-1,-1]
precision <- mat[2,2] / sum(mat[,2])
recall <- mat[2,2] / sum(mat[2,])
f1_score[i,j] <- 2 * (precision * recall) / (precision + recall)
}
}
seuil <- c(0.1, 0.3, 0.5)
treshold <- c(0.15, 0.2, 0.22)
## Séparation des données d'entrainement
k <- sample(1:nrow(dat.pca),0.7 * nrow(dat.pca), replace = F)
train <- dat.pca[k,]
valid <- dat.pca[-k,]
f1_score <- matrix(NA, nrow = length(seuil), ncol = length(treshold))
for (i in length(seuil))
seuil <- c(0.1, 0.3, 0.5)
treshold <- c(0.15, 0.2, 0.22)
## Séparation des données d'entrainement
k <- sample(1:nrow(dat.pca),0.7 * nrow(dat.pca), replace = F)
train <- dat.pca[k,]
valid <- dat.pca[-k,]
f1_score <- matrix(NA, nrow = length(seuil), ncol = length(treshold))
for (i in length(seuil))
{
for (j in length(treshold))
{
dat.pca <- data.frame(Ind = toitvert$green_roof_ind,
pca.x[, which(pca.summary$importance[3,] <= seuil[i])])
model.str <- 'Ind~.'
model.pca <- glm(as.formula(mod.str), data = dat.pca, family = binomial(link = 'logit'))
summary(model.pca) ; anova(model.pca)
train.mod <- glm(as.formula(mod.str), data = train, family = binomial(link = 'logit'))
## Calcul des prédictions et de la conversion
valid$Pred <- predict(train.mod, newdata = valid, type = 'response')
valid$validbin <- ifelse(valid$Pred >= treshold[j], 1, 0)
## Générer la matrice des confusions et calculer les statistiques pertinentes
confusion_mat <- confusionMatrixFor_Neg1_0_1(valid$Ind, valid$validbin)[-1,-1]
precision <- mat[2,2] / sum(mat[,2])
recall <- mat[2,2] / sum(mat[2,])
f1_score[i,j] <- 2 * (precision * recall) / (precision + recall)
}
}
f1_score
seq_along(seuil)
length(seuil)
seq_along(seuil)
seuil <- c(0.1, 0.3, 0.5)
treshold <- c(0.15, 0.2, 0.22)
## Séparation des données d'entrainement
k <- sample(1:nrow(dat.pca),0.7 * nrow(dat.pca), replace = F)
train <- dat.pca[k,]
valid <- dat.pca[-k,]
f1_score <- matrix(NA, nrow = length(seuil), ncol = length(treshold))
for (i in seq_along(seuil))
{
for (j in seq_along(treshold))
{
dat.pca <- data.frame(Ind = toitvert$green_roof_ind,
pca.x[, which(pca.summary$importance[3,] <= seuil[i])])
model.pca <- glm(Ind~., data = dat.pca, family = binomial(link = 'logit'))
summary(model.pca) ; anova(model.pca)
train.mod <- glm(Ind~., data = train, family = binomial(link = 'logit'))
## Calcul des prédictions et de la conversion
valid$Pred <- predict(train.mod, newdata = valid, type = 'response')
valid$validbin <- ifelse(valid$Pred >= treshold[j], 1, 0)
## Générer la matrice des confusions et calculer les statistiques pertinentes
confusion_mat <- confusionMatrixFor_Neg1_0_1(valid$Ind, valid$validbin)[-1,-1]
precision <- mat[2,2] / sum(mat[,2])
recall <- mat[2,2] / sum(mat[2,])
f1_score[i,j] <- 2 * (precision * recall) / (precision + recall)
}
}
f1_score
for (i in seq_along(seuil))
{
for (j in seq_along(treshold))
{
dat.pca <- data.frame(Ind = toitvert$green_roof_ind,
pca.x[, which(pca.summary$importance[3,] <= seuil[i])])
model.pca <- glm(Ind~., data = dat.pca, family = binomial(link = 'logit'))
summary(model.pca) ; anova(model.pca)
train.mod <- glm(Ind~., data = train, family = binomial(link = 'logit'))
## Calcul des prédictions et de la conversion
valid$Pred <- predict(train.mod, newdata = valid, type = 'response')
valid$validbin <- ifelse(valid$Pred >= treshold[j], 1, 0)
## Générer la matrice des confusions et calculer les statistiques pertinentes
confusion_mat <- confusionMatrixFor_Neg1_0_1(valid$Ind, valid$validbin)[-1,-1]
precision <- confusion_mat[2,2] / sum(confusion_mat[,2])
recall <- confusion_mat[2,2] / sum(confusion_mat[2,])
f1_score[i,j] <- 2 * (precision * recall) / (precision + recall)
}
}
f1_score
seuil[1]
for (i in seq_along(importance))
{
for (j in seq_along(treshold))
{
dat.pca <- data.frame(Ind = toitvert$green_roof_ind,
pca.x[, which(pca.summary$importance[3,] <= importance[i])])
model.pca <- glm(Ind~., data = dat.pca, family = binomial(link = 'logit'))
# summary(model.pca) ; anova(model.pca)
train <- dat.pca[k,]
valid <- dat.pca[-k,]
train.mod <- glm(Ind~., data = train, family = binomial(link = 'logit'))
## Calcul des prédictions et de la conversion
valid$Pred <- predict(train.mod, newdata = valid, type = 'response')
valid$validbin <- ifelse(valid$Pred >= treshold[j], 1, 0)
## Générer la matrice des confusions et calculer les statistiques pertinentes
confusion_mat <- confusionMatrixFor_Neg1_0_1(valid$Ind, valid$validbin)[-1,-1]
precision <- confusion_mat[2,2] / sum(confusion_mat[,2])
recall <- confusion_mat[2,2] / sum(confusion_mat[2,])
f1_score[i,j] <- 2 * (precision * recall) / (precision + recall)
}
}
importance <- c(0.1, 0.3, 0.5)
treshold <- c(0.15, 0.2, 0.22)
## Séparation des données d'entrainement
k <- sample(1:nrow(dat.pca),0.7 * nrow(dat.pca), replace = F)
f1_score <- matrix(NA, nrow = length(importance), ncol = length(treshold))
for (i in seq_along(importance))
{
for (j in seq_along(treshold))
{
dat.pca <- data.frame(Ind = toitvert$green_roof_ind,
pca.x[, which(pca.summary$importance[3,] <= importance[i])])
model.pca <- glm(Ind~., data = dat.pca, family = binomial(link = 'logit'))
# summary(model.pca) ; anova(model.pca)
train <- dat.pca[k,]
valid <- dat.pca[-k,]
train.mod <- glm(Ind~., data = train, family = binomial(link = 'logit'))
## Calcul des prédictions et de la conversion
valid$Pred <- predict(train.mod, newdata = valid, type = 'response')
valid$validbin <- ifelse(valid$Pred >= treshold[j], 1, 0)
## Générer la matrice des confusions et calculer les statistiques pertinentes
confusion_mat <- confusionMatrixFor_Neg1_0_1(valid$Ind, valid$validbin)[-1,-1]
precision <- confusion_mat[2,2] / sum(confusion_mat[,2])
recall <- confusion_mat[2,2] / sum(confusion_mat[2,])
f1_score[i,j] <- 2 * (precision * recall) / (precision + recall)
}
}
f1_score
colnames(f1_score) <- treshold
rownames(f1_score) <- importance
importance <- c(0.1, 0.3, 0.5)
treshold <- c(0.15, 0.2, 0.22)
## Séparation des données d'entrainement
k <- sample(1:nrow(dat.pca),0.7 * nrow(dat.pca), replace = F)
f1_score <- matrix(NA, nrow = length(importance), ncol = length(treshold))
colnames(f1_score) <- treshold
rownames(f1_score) <- importance
for (i in seq_along(importance))
{
for (j in seq_along(treshold))
{
dat.pca <- data.frame(Ind = toitvert$green_roof_ind,
pca.x[, which(pca.summary$importance[3,] <= importance[i])])
model.pca <- glm(Ind~., data = dat.pca, family = binomial(link = 'logit'))
# summary(model.pca) ; anova(model.pca)
train <- dat.pca[k,]
valid <- dat.pca[-k,]
train.mod <- glm(Ind~., data = train, family = binomial(link = 'logit'))
## Calcul des prédictions et de la conversion
valid$Pred <- predict(train.mod, newdata = valid, type = 'response')
valid$validbin <- ifelse(valid$Pred >= treshold[j], 1, 0)
## Générer la matrice des confusions et calculer les statistiques pertinentes
confusion_mat <- confusionMatrixFor_Neg1_0_1(valid$Ind, valid$validbin)[-1,-1]
precision <- confusion_mat[2,2] / sum(confusion_mat[,2])
recall <- confusion_mat[2,2] / sum(confusion_mat[2,])
f1_score[i,j] <- 2 * (precision * recall) / (precision + recall)
}
}
f1_score
## Définition des seuils
importance <- c(seq(0.1, 0.9, 0.1), seq(0.91, 0.99, 0.01))
treshold <- seq(0.15, 0.4, 0.01)
## Définition des seuils
importance <- c(seq(0.5, 0.9, 0.1), 0.95)
treshold <- seq(0.15, 0.4, 0.05)
## Définition des seuils
importance <- c(seq(0.5, 0.9, 0.1), 0.95)
treshold <- seq(0.15, 0.4, 0.05)
## Séparation des données d'entrainement
k <- sample(1:nrow(dat.pca),0.7 * nrow(dat.pca), replace = F)
f1_score <- matrix(NA, nrow = length(importance), ncol = length(treshold))
colnames(f1_score) <- treshold
rownames(f1_score) <- importance
for (i in seq_along(importance))
{
for (j in seq_along(treshold))
{
dat.pca <- data.frame(Ind = toitvert$green_roof_ind,
pca.x[, which(pca.summary$importance[3,] <= importance[i])])
model.pca <- glm(Ind~., data = dat.pca, family = binomial(link = 'logit'))
# summary(model.pca) ; anova(model.pca)
train <- dat.pca[k,]
valid <- dat.pca[-k,]
train.mod <- glm(Ind~., data = train, family = binomial(link = 'logit'))
## Calcul des prédictions et de la conversion
valid$Pred <- predict(train.mod, newdata = valid, type = 'response')
valid$validbin <- ifelse(valid$Pred >= treshold[j], 1, 0)
## Générer la matrice des confusions et calculer les statistiques pertinentes
confusion_mat <- confusionMatrixFor_Neg1_0_1(valid$Ind, valid$validbin)[-1,-1]
precision <- confusion_mat[2,2] / sum(confusion_mat[,2])
recall <- confusion_mat[2,2] / sum(confusion_mat[2,])
f1_score[i,j] <- 2 * (precision * recall) / (precision + recall)
}
}
f1_score
write.csv(f1_score, "f1_score.csv")
calcul_f1 <- function(importance, treshold)
{
for (i in seq_along(importance))
{
for (j in seq_along(treshold))
{
dat.pca <- data.frame(Ind = toitvert$green_roof_ind,
pca.x[, which(pca.summary$importance[3,] <= importance[i])])
model.pca <- glm(Ind~., data = dat.pca, family = binomial(link = 'logit'))
# summary(model.pca) ; anova(model.pca)
train <- dat.pca[k,]
valid <- dat.pca[-k,]
train.mod <- glm(Ind~., data = train, family = binomial(link = 'logit'))
## Calcul des prédictions et de la conversion
valid$Pred <- predict(train.mod, newdata = valid, type = 'response')
valid$validbin <- ifelse(valid$Pred >= treshold[j], 1, 0)
## Générer la matrice des confusions et calculer les statistiques pertinentes
confusion_mat <- confusionMatrixFor_Neg1_0_1(valid$Ind, valid$validbin)[-1,-1]
precision <- confusion_mat[2,2] / sum(confusion_mat[,2])
recall <- confusion_mat[2,2] / sum(confusion_mat[2,])
f1_score[i,j] <- 2 * (precision * recall) / (precision + recall)
}
f1_score
}
}
importance <- c(seq(0.5, 0.9, 0.1), 0.95)
treshold <- seq(0.15, 0.2, 0.05)
importance <- c(seq(0.5, 0.9, 0.1), 0.95)
treshold <- seq(0.15, 0.2, 0.01)
importance <- c(seq(0.5, 0.9, 0.1), 0.95)
treshold <- seq(0.15, 0.2, 0.01)
calcul_f1(importance, treshold)
calcul_f1 <- function(importance, treshold)
{
for (i in seq_along(importance))
{
for (j in seq_along(treshold))
{
dat.pca <- data.frame(Ind = toitvert$green_roof_ind,
pca.x[, which(pca.summary$importance[3,] <= importance[i])])
model.pca <- glm(Ind~., data = dat.pca, family = binomial(link = 'logit'))
# summary(model.pca) ; anova(model.pca)
train <- dat.pca[k,]
valid <- dat.pca[-k,]
train.mod <- glm(Ind~., data = train, family = binomial(link = 'logit'))
## Calcul des prédictions et de la conversion
valid$Pred <- predict(train.mod, newdata = valid, type = 'response')
valid$validbin <- ifelse(valid$Pred >= treshold[j], 1, 0)
## Générer la matrice des confusions et calculer les statistiques pertinentes
confusion_mat <- confusionMatrixFor_Neg1_0_1(valid$Ind, valid$validbin)[-1,-1]
precision <- confusion_mat[2,2] / sum(confusion_mat[,2])
recall <- confusion_mat[2,2] / sum(confusion_mat[2,])
f1_score[i,j] <- 2 * (precision * recall) / (precision + recall)
}
}
f1_score
}
importance <- c(seq(0.5, 0.9, 0.1), 0.95)
treshold <- seq(0.15, 0.4, 0.05)
##
importance <- c(seq(0.5, 0.9, 0.1), 0.95)
treshold <- seq(0.15, 0.2, 0.01)
calcul_f1(importance, treshold)
##
importance <- c(seq(0.5, 0.9, 0.1), 0.95)
##
importance <- c(seq(0.5, 0.9, 0.1), 0.95)
treshold <- seq(0.15, 0.2, 0.01)
calcul_f1(importance, treshold)
calcul_f1 <- function(importance, treshold)
{
colnames(f1_score) <- treshold
rownames(f1_score) <- importance
for (i in seq_along(importance))
{
for (j in seq_along(treshold))
{
dat.pca <- data.frame(Ind = toitvert$green_roof_ind,
pca.x[, which(pca.summary$importance[3,] <= importance[i])])
model.pca <- glm(Ind~., data = dat.pca, family = binomial(link = 'logit'))
# summary(model.pca) ; anova(model.pca)
train <- dat.pca[k,]
valid <- dat.pca[-k,]
train.mod <- glm(Ind~., data = train, family = binomial(link = 'logit'))
## Calcul des prédictions et de la conversion
valid$Pred <- predict(train.mod, newdata = valid, type = 'response')
valid$validbin <- ifelse(valid$Pred >= treshold[j], 1, 0)
## Générer la matrice des confusions et calculer les statistiques pertinentes
confusion_mat <- confusionMatrixFor_Neg1_0_1(valid$Ind, valid$validbin)[-1,-1]
precision <- confusion_mat[2,2] / sum(confusion_mat[,2])
recall <- confusion_mat[2,2] / sum(confusion_mat[2,])
f1_score[i,j] <- 2 * (precision * recall) / (precision + recall)
}
}
f1_score
}
##
importance <- c(seq(0.5, 0.9, 0.1), 0.95)
treshold <- seq(0.15, 0.2, 0.01)
calcul_f1(importance, treshold)
## modèle avec meilleur F1_score :
treshold.final <- 0.17
importance.final <- 0.8
## modèle avec meilleur F1_score :
treshold_final <- 0.17
importance_final <- 0.8
f1_final <- calcul_f1(treshold_final, importance_final)
f1_final <- 0.387931
##
dat.pca_final <- data.frame(Ind = toitvert$green_roof_ind,
pca.x[, which(pca.summary$importance[3,] <= 0.8)])
mod_final <- glm(Ind~., data = dat.pca_final, family = binomial(link = 'logit'))
plot(mod_final$fitted.values, toitvert$green_roof_ind)
confusion_mat
confusion_mat / nrow(dat)
confusion_mat / nrow(dat.pca_final)
nrow(mod_final$fitted.values)
mod_final <- glm(Ind~., data = dat.pca_final, family = binomial(link = 'logit'))
nrow(mod_final$fitted.values)
##
dat.pca_final <- data.frame(Ind = toitvert$green_roof_ind,
pca.x[, which(pca.summary$importance[3,] <= 0.8)])
mod_final <- glm(Ind~., data = dat.pca_final, family = binomial(link = 'logit'))
nrow(mod_final$fitted.values)
mod_final$fitted.values
length(mod_final$fitted.values)
conf_mat_final <- confusionMatrixFor_Neg1_0_1(dat.pca_final$Ind,
mod_final$fitted.values)
length(dat.pca_final$Ind)
length(mod_final$fitted.values)
predictions_final <- ifelse(mod_final$fitted.values>= treshold_final, 1, 0)
predictions_final <- ifelse(mod_final$fitted.values>= treshold_final, 1, 0)
conf_mat_final <- confusionMatrixFor_Neg1_0_1(dat.pca_final$Ind,
predictions_final)
confusion_mat / length(mod_final$fitted.values)
conf_mat_final / length(mod_final$fitted.values)
conf_mat_final / length(mod_final$fitted.values)[-1,-1]
conf_mat_final[-1,-1] / length(mod_final$fitted.values)
##
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
## Load dependencies
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
## Load dependencies
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
source('dependencies.R')
## Load dependencies
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
source('dependencies.R')
## Load dependencies
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
source('dependencies.R')
runApp()
runApp()
runApp()
runApp()
runApp()
body <- dashboardBody(
tabItems(
tabItem(
tabName = "data_analysis",
box(title = 'Analyse des données'),
box(title = "Analyse en composantes principales",
width = 12, collapsible = T, collapsed = T,
plotOutput('custom_plot'))
),
tabItem(
tabName = 'regression_lineaire',
fluidRow( # avec les verbatim, il faut wrap dans fluidrow
box(title = "Optimisation du score F1",
collapsed = T, collapsible = T,
png("f1_score.png")),
box(title = 'Matrice de confusion et score F1',
status = 'info', solidHeader = T,
collapsible = T, collapsed = F,
width = 12,
verbatimTextOutput(
'matrice'
),
h3("Treshold et seuil d'importance optimaux"),
p("Treshold : 0.17"),
p("Seuil d'importance : 0.8")
))
)
)
)
runApp()
rm(list = ls())
load('environement')
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp('/Volumes/NO NAME/src')
runApp('/Volumes/NO NAME/src')
shiny::runApp()
runApp()
runApp()
